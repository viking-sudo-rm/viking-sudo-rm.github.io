<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">
<!-- Avoid warning on Google Chrome
        Error with Permissions-Policy header: Origin trial controlled feature not enabled: 'interest-cohort'.
        see https://stackoverflow.com/a/75119417
    -->
    <meta http-equiv="Permissions-Policy" content="interest-cohort=()">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Theory of Saturated Neural Networks | William  Merrill</title>
    <meta name="author" content="William  Merrill">
    <meta name="description" content="Summarizing &lt;i&gt;Sequential Neural Networks as Automata&lt;/i&gt;">
    <meta name="keywords" content="artificial-intelligence, theory-of-computation, deep-learning, language-models">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="/assets/img/icon.png">
    
    <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
    <link rel="canonical" href="http://localhost:4000/blog/2019/theory-of-saturated-neural-networks/">

    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Theory of Saturated Neural Networks",
      "description": "Summarizing <i>Sequential Neural Networks as Automata</i>",
      "published": "September 6, 2019",
      "authors": [
        {
          "author": "William Merrill",
          "authorURL": "/",
          "affiliations": [
            {
              "name": "AI2",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">William </span>Merrill</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/cv">CV</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Theory of Saturated Neural Networks</h1>
        <p>Summarizing <i>Sequential Neural Networks as Automata</i></p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        

        <p>The main idea of my paper <a href="https://arxiv.org/abs/1906.01615" rel="external nofollow noopener" target="_blank">Sequential Neural Networks as Automata</a> is that, if we making some simplifying assumptions about how neural networks work, we can derive a theory of network expressiveness (what formal languages can different architectures model?) that seems to agree with the classes of formal languages that different networks can learn when trained by gradient descent. Thus, this restricted theoretical capacity seems to be (potentially) a good proxy for the empirical learnable capacity of various networks.</p>

<h2 id="saturated-networks">Saturated Networks</h2>

<p>In the paper, I referred to the simplified network whose capacity we can analyze as an <em>asymptotic</em> network. However, after talking with Gail Weiss, I now believe the term <em>saturated</em> is more descriptive, and plan to use this term going forward.</p>

<p>A neural network is a function $f(x, \theta)$ that is almost-everywhere differentiable with respect to the parameters $\theta$. Given such a function, we derive the saturated network $f’$ as</p>

\[f'(x, \theta) = \lim_{N \rightarrow \infty} f(x, N\theta) .\]

<p>We define $ f’(x, \theta) $ over the domain of $(x, \theta)$ for which the limit above exists.</p>

<p>In a neural network, the effect of this transformation is to discretize all of the activations. For example, consider a neuron:</p>

\[\sigma(wx + b)\]

<p>where $\sigma$ is the sigmoid function. When we take the limit of $\sigma(Nwx + Nb)$, the output of the neuron approaches either $0$ or $1$. This is what I mean by <em>discretization</em>.</p>

<p>After applying this discretization to the full network, we can analyze the computational capacity of the resulting discrete automaton. We also define a notion of space complexity associated with these saturated networks in the paper. Intuitively, this measure of complexity is just the number of configurations that the saturated network can have after reading a sequence of length $n$. For more details on this, consult <a href="https://arxiv.org/abs/1906.01615" rel="external nofollow noopener" target="_blank">the paper</a>.</p>

<h2 id="summary-of-results">Summary of Results</h2>

<p>By $L(M)$, we denote the set of formal languages that a machine $M$ can accept. Some key capacity results from the paper are as follows:</p>

<ul>
  <li>$L(\textrm{ConvNet})$ is a proper subset of the regular languages</li>
  <li>$L(\textrm{RNN})$ is exactly the regular languages</li>
  <li>$L(\textrm{GRU})$ is exactly the regular languages</li>
  <li>$L(\textrm{LSTM})$ is a superset of the regular languages, and a subset of the real-time counter languages</li>
</ul>

<p>The core results about the configuration complexity, some of which are analogous, are:</p>

<ul>
  <li>ConvNet has $O(1)$ configurations</li>
  <li>RNN has $O(1)$ configurations</li>
  <li>GRU has $O(1)$ configurations</li>
  <li>LSTM has $O(n^k)$ configurations for hidden size $k$</li>
  <li>Attention has $2^{O(n)}$ configurations</li>
  <li>
<a href="https://github.com/viking-sudo-rm/stacknn-core" rel="external nofollow noopener" target="_blank">StackNN</a> has $2^{O(n)}$ configurations</li>
</ul>

<p>There are some other results about attention/transformers that I’m not going to get into here, since they’re not so neat. If you’re interested though, I refer you to Section 4 of <a href="https://arxiv.org/abs/1906.01615" rel="external nofollow noopener" target="_blank">the paper</a>.</p>

<h2 id="empirical-evidence">Empirical Evidence</h2>

<h3 id="positive-evidence">Positive Evidence</h3>

<p>The development of this theory was motivated by past empirical results about what RNNs are able to learn. Two types of tasks (counting and reversing) serve as relevant diagnostics for assessing the computational power of different architectures.</p>

<p><a href="https://arxiv.org/abs/1805.04908" rel="external nofollow noopener" target="_blank">Weiss et al.</a> show that LSTMs can learn to count, whereas RNNs and GRUs do not. Similarly, <a href="https://arxiv.org/abs/1906.03648" rel="external nofollow noopener" target="_blank">Sugzgun et al.</a> observe that LSTMs can learn the 1-Dyck counter language, whereas other RNNs do not. This is predicted by the theory since saturated LSTMs can do counting, but saturated RNNs and GRUs are finite state.</p>

<p><a href="https://arxiv.org/abs/1809.02836" rel="external nofollow noopener" target="_blank">Hao et al.</a> show how stack neural networks can solve the (beyond counter language) task of reversing a string, whereas LSTMs fail badly on this. In my paper, I also showed that attention can solve this task. Both of the results are predicted by the theory, since stack neural networks and attention allow for exponential configurations, which are needed to reverse a string, whereas LSTMs are limited to polynomial configurations.</p>

<h3 id="negative-evidence">Negative Evidence</h3>

<p>While I found that regularized neural networks display the counting pattern reported by <a href="https://arxiv.org/abs/1805.04908" rel="external nofollow noopener" target="_blank">Weiss et al.</a>, I also found that the unregularized LSTM, GRU, and RNN can all learn to model the language $a^nb^nc$, which requires counting. Thus, it might be more precise to say that the saturated theory seems to describe the learnable capacity of <em>regularized</em> networks. One possible interpretation of this is that the constraints imposed by regularization prevent the network from learning strategies beyond the saturated capacity.</p>

<h2 id="proof-sketches">Proof Sketches</h2>

<h3 id="rnn-capacity-and-complexity">RNN Capacity and Complexity</h3>

<p>To show that $L(\textrm{RNN})$ is the regular languages, we show two directions of containment.</p>

<p>First, we prove that the the regular languages are an upper bound. We do this by showing that the configuration complexity of the RNN is finite, i.e. $O(1)$. Since each neuron has two possible values ($-1$ and $1$), and there are $k$ neurons in the state, the number of configurations of the state vector is $O(2^k) = O(1)$.</p>

<p>The other direction is a little more complicated. We need to construct an RNN to simulate an arbitrary finite state machine. A construction for this is provided in Lemma B.1.</p>

<h3 id="lstm-capacity-and-complexity">LSTM Capacity and Complexity</h3>

<p>In the LSTM case, even when we discretize the network, we get a model with more than finite state. This is because the LSTM’s gating architecture is capable of counting (<a href="https://arxiv.org/abs/1805.04908" rel="external nofollow noopener" target="_blank">Weiss et al.</a>, 2018).</p>

<p>To show that the counter languages are an upper bound, we write the saturated gate network for a particular counter state neuron $c$ as follows:</p>

<!-- For some reason, the normal \lim_ is not working here. Neither does f_tc_{t-1}. I guess -->

\[\underset{N \rightarrow \infty}{\lim} c_t\]

\[= \underset{N \rightarrow \infty}{\lim} fc_{t-1} + i{\tilde c}_t\]

\[= \underset{N \rightarrow \infty}{\lim} ac_{t-1} + b\]

<p>where $a$ is $0$ or $1$ and $b$ is $-1$, $0$, or $1$. This equation parameterizes a real-time counter machine update. Thus, the counter languages are an upper bound on the saturated LSTM capacity.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography>
</div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 William  Merrill. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme and inspired by <a href="https://gbrlfaria.github.io/" rel="external nofollow noopener" target="_blank">Gabriel Faria's website</a>. <br>Photo by Brian Kitano.
Last updated: October 01, 2025.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
