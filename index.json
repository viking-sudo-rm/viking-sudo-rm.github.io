[{"authors":["admin"],"categories":null,"content":"I am an incoming research resident on the AllenNLP team at AI2. I recently graduated from Yale, where my thesis was advised by Bob Frank and Dana Angluin. My research interests lie at the intersection of language and computation. In particular, my recent work has developed theory to understand NLP deep learning models. I am also interested in the practical potential of NLP for medical discoveries, studying language change, and many other applications.\n","date":1572048000,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1572048000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://lambdaviking.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am an incoming research resident on the AllenNLP team at AI2. I recently graduated from Yale, where my thesis was advised by Bob Frank and Dana Angluin. My research interests lie at the intersection of language and computation. In particular, my recent work has developed theory to understand NLP deep learning models. I am also interested in the practical potential of NLP for medical discoveries, studying language change, and many other applications.","tags":null,"title":"Will Merrill","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"http://lambdaviking.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"http://lambdaviking.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"http://lambdaviking.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"http://lambdaviking.com/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Will Merrill"],"categories":["NLP"],"content":" The main idea of my paper Sequential Neural Networks as Automata is that, if we making some simplifying assumptions about how neural networks work, we can derive a theory of network expressiveness (what formal languages can different architectures model?) that seems to agree with the classes of formal languages that different networks can learn when trained by gradient descent. Thus, this restricted theoretical capacity seems to be (potentially) a good proxy for the empirical learnable capacity of various networks.\nSaturated Networks In the paper, I referred to the simplified network whose capacity we can analyze as an asymptotic network. However, after talking with Gail Weiss, I now believe the term saturated is more descriptive, and plan to use this term going forward.\nA neural network is a function $f(x, \\theta)$ that is almost-everywhere differentiable with respect to the parameters $\\theta$. Given such a function, we derive the saturated network $f\u0026rsquo;$ as\n$$ f\u0026rsquo;(x, \\theta) = \\lim_{N \\rightarrow \\infty} f(x, N\\theta) . $$\nWe define $f\u0026rsquo;$ over the domain of $(x, \\theta)$ for which the limit above exists. Since $f$ is almost-everwhere differentiable, the set of points excluded from the domain is measure-zero.\nIn a neural network, the effect of this transformation is to discretize all of the activations. For example, consider a neuron:\n$$ \\sigma(wx + b) $$\nwhere $\\sigma$ is the sigmoid function. When we take the limit of $\\sigma(Nwx + Nb)$, the output of the neuron approaches either $0$ or $1$. This is what I mean by discretization.\nAfter applying this discretization to the full network, we can analyze the computational capacity of the resulting discrete automaton. We also define a notion of space complexity associated with these saturated networks in the paper. Intuitively, this measure of complexity is just the number of configurations that the saturated network can have after reading a sequence of length $n$. For more details on this, consult the paper.\nSummary of Results By $L(M)$, we denote the set of formal languages that a machine $M$ can accept. Some key capacity results from the paper are as follows:\n $L(\\textrm{ConvNet})$ is a proper subset of the regular languages $L(\\textrm{RNN})$ is exactly the regular languages $L(\\textrm{GRU})$ is exactly the regular languages $L(\\textrm{LSTM})$ is a superset of the regular languages, and a subset of the real-time counter languages  The core results about the configuration complexity, some of which are analogous, are:\n ConvNet has $O(1)$ configurations RNN has $O(1)$ configurations GRU has $O(1)$ configurations LSTM has $O(n^k)$ configurations for hidden size $k$ Attention has $2^{O(n)}$ configurations StackNN has $2^{O(n)}$ configurations  There are some other results about attention/transformers that I\u0026rsquo;m not going to get into here, since they\u0026rsquo;re not so neat. If you\u0026rsquo;re interested though, I refer you to Section 4 of the paper.\nEmpirical Evidence Positive Evidence The development of this theory was motivated by past empirical results about what RNNs are able to learn. Two types of tasks (counting and reversing) serve as relevant diagnostics for assessing the computational power of different architectures.\nWeiss et al. show that LSTMs can learn to count, whereas RNNs and GRUs do not. Similarly, Sugzgun et al. observe that LSTMs can learn the 1-Dyck counter language, whereas other RNNs do not. This is predicted by the theory since saturated LSTMs can do counting, but saturated RNNs and GRUs are finite state.\nHao et al. show how stack neural networks can solve the (beyond counter language) task of reversing a string, whereas LSTMs fail badly on this. In my paper, I also showed that attention can solve this task. Both of the results are predicted by the theory, since stack neural networks and attention allow for exponential configurations, which are needed to reverse a string, whereas LSTMs are limited to polynomial configurations.\nNegative Evidence While I find that regularized neural networks display the counting pattern reported by Weiss et al., I also find that the unregularized LSTM, GRU, and RNN can all learn to model the language $a^nb^nc$, which requires counting. Thus, it might be more precise to say that the saturated theory seems to describe the learnable capacity of regularized networks. One possible interpretation of this is that the constraints imposed by regularization prevent the network from learning strategies beyond the saturated capacity.\nProof Sketches RNN Capacity and Complexity To show that $L(\\textrm{RNN})$ is the regular languages, we show two directions of containment.\nFirst, we prove that the the regular languages are an upper bound. We do this by showing that the configuration complexity of the RNN is finite, i.e. $O(1)$. Since each neuron has two possible values ($-1$ and $1$), and there are $k$ neurons in the state, the number of configurations of the state vector is $O(1)$.\nThe other direction is a little more complicated. We need to construct an RNN to simulate an arbitrary finite state machine. A construction for this is provided in Lemma B.1.\nLSTM Capacity and Complexity In the LSTM case, even when we discretize the network, we get a model with more than finite state. This is because the LSTM\u0026rsquo;s gating architecture is capable of counting (Weiss et al., 2018).\nTo show that the counter languages are an upper bound, we write the saturated gate network for a particular counter state neuron $c$ as follows:\n$$ \\underset{N \\rightarrow \\infty}{\\lim} c_t $$\n$$ = \\underset{N \\rightarrow \\infty}{\\lim} fc_{t-1} + i{\\tilde c}_t $$\n$$ = \\underset{N \\rightarrow \\infty}{\\lim} ac_{t-1} + b $$\nwhere $a$ is $0$ or $1$ and $b$ is $-1$, $0$, or $1$. With unrestricted $a$ and $b$, this update parameterized a real-time counter machine update. Thus, the counter languages are an upper bound on the saturated LSTM capacity.\n","date":1572048000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572048000,"objectID":"726ef2deb6c88139675f79dcdb7e8768","permalink":"http://lambdaviking.com/post/saturated-networks/","publishdate":"2019-10-26T00:00:00Z","relpermalink":"/post/saturated-networks/","section":"post","summary":"Summarizing Sequential Neural Networks as Automata.","tags":null,"title":"Theory of Saturated Neural Networks","type":"post"},{"authors":["Will Merrill"],"categories":["NLP"],"content":" This post is a summary of my thoughts and experiences at ACL 2019, focused around the theme of interpretability. After spending most of the summer thinking about things besides neural networks, I was abruptly thrown back into things upon arriving in Florence. I spent a lot of my downtime with friends from Yale that I was staying with. On the other hand, I also met my future colleagues at AI2 and many other young researchers who are interested in similar issues of the interpretability and formal analysis of neural networks. In this way, the conference felt like a soft threshold between old and new.\nThere were plenty of exciting things going on at the main conference, from work on incorporated graph representations into transformers to the development of more energy-efficient non-autoregressive NLP models. In this post, though, I\u0026rsquo;m going to focus on parts that I was most directly involved in, which were the Blackbox NLP workshop on interpretability and the Deep Learning and Formal Languages (DELFOL) workshop.\nBlackbox NLP This was the second iteration of the Blackbox workshop. While the last Blackbox at EMNLP was also quite large and interesting, it had felt to me like a quite interdisciplinary meeting of researchers doing very different things. In contrast, I got the impression at this year\u0026rsquo;s meeting that a community of interpretability-minded researchers with similar goals and methods has formed. This is consistent with the wider interest that interpretability has sparked in the larger NLP community and the number of interpretability papers presented in the main conference. In the closing panel of the workshop, it was mentioned that perhaps interpretability would become a session within the conference at the next ACL meeting instead of a supplementary workshop.\nOne of the main goals of interpretability research is to assess what kinds of information is encoded in different representations within a neural network. In particular, people (including myself) are interested in how different models encode grammatical information. Several methods were discussed for addressing this question, including attention, probing, representational similarity analysis (RSA).\nAttention A classical way of interpreting whether networks encode grammatical information is by looking at attention alignments. For example, we might expect that, if there is a dependency between words $i$ and $j$, then position $i$ in the network should attend to position $j$. While this probably works to some degree, attention maps can often get messy. Also, as Serrano et al. pointed out in their work at the main conference, perturbing the attention distribution has surprisingly little effect on a network\u0026rsquo;s behavior. Thus, we might want to look to other methods for interpreting neural representations.\nProbing Probing refers to using a linear classifier to try to recover some kind of information, say part of speech information, from some representation within a model. This has been a standard way of interpreting neural representations throughout the last year. The underlying assumption is that, if information is recoverable in a single linear layer, then it is probably strongly encoded in the representation. As was pointed out to me by Jungo Kasai, one pitfall of this method is that it just assesses whether or not the information of interest is encoded in a representation, not to what degree it is there. For example, if a small amount of the representation in some network is representing grammatical information, whereas the rest of it is representing spurious correlations between words, probing would still suggest that that layer is encoding grammar. In such a case, is it really reasonable to conclude that a grammatical representation is an important part of the behavior which the network is learning?\nRSA An alternate approach to the same problem is RSA, which is not to be confused with the cryptographic protocol of the same name. RSA sidesteps the issue with probing by measuring the degree to which two representations are correlated, not whether the information from one can be recovered from the other.\nFormally, RSA is a method of computing the similarity between two representations $X$ and $Y$. We assume that we do not have the direct ability to compare $x$ to $y$, but instead have two internally defined similarity measures $d_X(x_1, x_2)$ and $d_Y(y_1, y_2)$. These measures need not be symmetric.\nTo get an overall measure of similarity, we compute a similar matrices $M(X)$ and $M(Y)$ where\n$$ M(X)_{ij} = d_X(x_i, x_j) $$\nand analogously for $Y$. Once we have these matrices, we compute an overall measure of similarity by taking the correlation between them. I find RSA to be a quite exciting method for future research, as it avoids some of the pitfalls of probing while also being very easy (perhaps easier than probing) to implement. This is not to say that probing should be abandoned completely, as being able to verify the same results with different methods increases their confidence. I recall hearing a talk at ACL in which RSA was already applied to transformer representations and replicated probing findings, although I unfortunately cannot find the paper which reported this.\nConclusion Probing and RSA are two good methods for understanding what information is encoded in some representation within a trained network. This is still a slightly different question than asking what information is particular relevant for the decision that a network makes given some input, which can be addressed using gradient methods or perturbation studies. I think that interpreting representations and interpreting decisions are both interesting questions.\nAdditionally, both of these questions have analogs in the formal domain, where we can analyze what representations different neural network hidden states can encode, or what classes of formal languages a certain architecture can accept. As others at ACL also mentioned, I would like to see future work connecting empirical interpretability research to the formal analysis that has been done.\nFinally, as many people expressed during the conference, I think that a good goal for the next year is progress towards constructive interpretability work. In other words, our interpretation of networks should allow us to make smarter architectures.\nDELFOL DELFOL was the venue for formal work at ACL 2019. Given its somewhat esoteric topic, this workshop was quite small, but I found it really exciting. To me, the following three questions captured a lot of the work that people were presenting:\n Assess the learnable capacity of neural network models using empirical studies Identify formal properties of neural architectures that lead to different kinds of inductive bias or learnable/representational capacity \u0026ldquo;Distill\u0026rdquo; neural network models into smaller, more interpretable discrete models  Empirical Experiments about Learnability On the empirical front, Suzgun et al. reported that LSTMs generalizably learn to model the 1-Dyck language, which consists of parenthetic expressions with one type of parentheses. Other types of RNNs, on the other hand, were not able to do this. Additionally, LSTMs could not model 2-Dyck, but could model independently mixed versions of 1-Dyck.\nSince 1-Dyck is a counter language, this is further evidence that LSTMs can count, unlike other kinds of RNNs. Additionally, since 2-Dyck is a context-free language that requires more memory than is available to counter machines, we have further experimental evidence that LSTM memory, like counter machine memory, is not sufficient for complex hierarchical representations. As Bob Frank summarized in his invited talk at DELFOL, LSTMs also struggle to do other memory intensive context-free things, like reversing strings.\nWhile my paper at DELFOL was mostly theoretical results, I also had some experimental stuff that agreed with these conclusions.\nFormal Analysis of Inductive Bias and Capacity Saturated Networks Building on work done by Weiss et al., my paper at DELFOL developed a theory of saturated networks, or networks where the activation functions are restricted to their asymptotic values instead of intermediate rational values. I proved that saturated RNNs and GRUs are equivalent to finite-state automata in capacity, whereas saturated LSTMs are equivalent to a restricted class of counter machines. Bob Frank also synthesized my results and Weiss et al.\u0026rsquo;s results in his invited talk.\nI think these theoretical results about saturated networks are interesting because they agree with empirical results (see previous section) about the types of languages that natural networks are empirically able to learn and generalize. This suggests that the capacity of the saturated network might represent something like the inductive bias or effective learnable capacity of the general network.\nOther Formal Properties In his talk, Bob Frank also mentioned other ways of understanding the abilities of different networks. In particular, he mentioned studying the closure properties of representable or learnable languages as a potential way to build a theory. Another notable formal property that he mentioned was semilinearity. In his talk, Noah Smith mentioned rational recurrence as one way to analyze different neural architectures. While less linguistically motivated, this property is nice because of its connections to classical finite-state methods in NLP and the interpretability that comes along with this.\nDistillation Finally, a general theme at DELFOL that I found especially interesting was distillation. This refers to converting a neural network to a smaller and more interpretable discrete model which approximates the same behavior. This has a variety of practical benefits: deployment on smaller devices, interpretability, and possibly regularization, to name a few. I think one of the reasons why I find this topic interesting is because it is a way for theory to provide very concrete solutions to practical problems.\nAs I see it, there are two main ways that distillation has been done. Weiss et al. pioneered the query-based approach, which involves using the L* algorithm to learn a finite-state machine using black-box queries to a neural network. The other type of distillation is spectral, which RÃ©mi Eyraud spoke about at DELFOL. This involves building a Hankel matrix for some language and then using SVD to extract a smaller matrix representing a weighted finite-state automaton. While these are the two methods that have been investigated, I\u0026rsquo;m interested in other ways to achieve distillation, and I have been doing some preliminary experiments using other methods. More on that to come!\n","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"b7b70d6795e06120ca33381b207f8776","permalink":"http://lambdaviking.com/post/the-state-of-interpretability-in-nlp/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/post/the-state-of-interpretability-in-nlp/","section":"post","summary":"Thoughts from my experience at ACL 2019.","tags":null,"title":"The State of Interpretability in NLP","type":"post"},{"authors":["Will Merrill","Gigi Felice Stark","Robert Frank"],"categories":null,"content":"","date":1564704120,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704120,"objectID":"7fd1d360a3be4679f6f06b13c322668c","permalink":"http://lambdaviking.com/publication/detecting-syntactic-change-using-a-neural-part-of-speech-tagger/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/detecting-syntactic-change-using-a-neural-part-of-speech-tagger/","section":"publication","summary":"We find that a neural network part-of-speech tagger implicity learns to model syntactic change.","tags":["Source Themes"],"title":"Detecting Syntactic Change Using a Neural Part-of-Speech Tagger","type":"publication"},{"authors":["Will Merrill"],"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"4dc25a7ddd220d46a3d5d5f1e20cfc12","permalink":"http://lambdaviking.com/publication/sequential-neural-networks-as-automata/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/sequential-neural-networks-as-automata/","section":"publication","summary":"I formally characterize the capacity and memory of various RNNs, given asymptotic assumptions.","tags":["Source Themes"],"title":"Sequential Neural Networks as Automata","type":"publication"},{"authors":["Will Merrill","Lenny Khazan","Noah Amsel","Yiding Hao","Simon Mendelsohn","Robert Frank"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"3c23164769232bbbcd435971c80f987a","permalink":"http://lambdaviking.com/publication/finding-syntactic-representations-in-neural-stacks/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/finding-syntactic-representations-in-neural-stacks/","section":"publication","summary":"Stack-augmented RNNs that are trained to perform language modeling learn to exploit linguistic structure without any supervision.","tags":["Source Themes"],"title":"Finding Syntactic Representations in Neural Stacks","type":"publication"},{"authors":["Will Merrill"],"categories":["Translation","Old English"],"content":"Often the outcast\nawaits his grace,\nthe Lord\u0026rsquo;s mercy,\nthough he, sorrow-minded,\nmust turn his oars\nalong the waters,\nwade the path of exile.\nFate goes always as it must!\nSo spoke the earth-stepper,\nmindful of his misfortunes,\nof wrathful slaughter,\nthe death of his comrades:\nOften in the morning,\nI had to discuss\nmy every trouble alone.\nThere is no one living\nthat I dare open my innermost thought to.\nTruly I know\nthat it ought to be among men,\nthe noble custom,\nwhere one secures the chest of his spirit,\nhordes his treasure case,\nthinks and speaks freely.\nThe weary spirit cannot\nwithstand fate,\nnor does the bitter mind\nhelp do any good.\nSo the glory-yearning\noften lock away their dreary\nthoughts in their breast-coffins.\nSo I,\noften wretched\nand besorrowed,\nhave had to seal away\nmy thoughts from kinsmen,\nsince, long ago,\nI sealed my lord\nin the darkness of the earth\nand from there travelled\nwinter-minded\nover the frozen waves,\nsought, hall-lacking,\na treasure-giver,\nwherever far or near\nI might find someone\nin the meadhall\nwho knew me,\nor was willing to console\nme, the friendless one,\nwow me with wonders.\nHe who has known it understands\nhow cruel sorrow is as a companion\nfor the one who has\nfew beloved friends.\nExile wraps him,\nnot rings of gold,\na frozen spirit,\nnot the prosperity of the earth.\nHe remembers hall-warriors\nand ring-givers,\nhow his gold-giver\ntreated him to feasting\nin his youth.\nHow all that joy has died!\nAnd so he knows this,\nhe who must for a long time\nforgo the counsel of his\ndear lord,\nwhen sorrow and sleep together\nwrap him up and bind him.\nHe dreams in his head\nthat he hugs and kisses\nhis lord,\nand on his lord\u0026rsquo;s knee\nlie his hand and head,\njust as he, at times\nin days gone by,\nenjoyed the gift throne.\nThen he wakes back up\na lordless man,\nhe sees before him\ndusky waves,\nsea birds bathing,\npruning their feathers,\nfrost and snow falling,\nmixed in with hail.\nThen are that much heavier\nthe wounds of his heart,\ngrieving after his lord.\nHis sorrow is renewed\nwhen his mind happens upon\nthe memory of his kinsmen.\nHe greets them with joy,\neagerly looks over\nthe gathering of men -\nbut they always swim away.\nThe journey of sailors\nnever brings back many\nknown stories.\nGrief is renewed\nfor him who must\nalways send his\nweary heart\nover the binding of the waves.\nTherefore I can\u0026rsquo;t think\nwhy my heart does not\ndarken when I reflect\non all the lives of men\nthroughout this world,\nhow they suddenly\nleft the hall floor,\nthose proud thanes.\nSo each day this Middle Earth\ncrumbles and decays.\nTherefore a man cannot become\nwise before he has a share\nof winters in the world.\nA wise man must be patient,\nmust not be too reckless,\nnor too word-hasty\nnor too weak a warrior,\nnor too rash,\nneither too fearful nor cheerful,\nnor too greedy of goods,\nand never too eager for boasting\nbefore he truly understands.\nA man must wait\nwhen he speaks oaths\nuntil the bold one\ntruly knows\nwhere his heart\u0026rsquo;s intent\nwill turn.\nA wise hero must realize\nhow ghastly it will be\nwhen all the wealth of this world\nlies waste,\nas now in various places\naround this earth,\nwalls stand\nblown by wind,\ncovered in snow,\nstorm-wrecked ruins.\nThe halls decay,\ntheir leader lies dead,\ndeprived of joy.\nThe whole troop\nhas proudly fallen by the wall.\nSome war took them,\ncarried them on their way.\nOne, a bird snatched off\nover the deep sea.\nAnother, the grey wolf\ntore apart after his death.\nAnother, buried\ndreary-faced\nin his grave.\nAnd so He destroyed this place,\nthe Creator of men,\nuntil missing the noise\nof its residents,\nthe old work of giants\nstood idle.\nHe who wisely reflected\non this ruin,\nand deeply pondered\nthis dark life,\nwise in experience,\ndistantly remembered\nmuch slaying,\nand spoke these words:\nWhere is the horse? Where is the rider?\nWhere is the treasure-giver?\nWhere are the hall\u0026rsquo;s joys?\nAlas, the bright cups!\nAlas, the warrior in mail!\nHow that time has departed,\ndark under night\u0026rsquo;s helm,\nas if it never were.\nNow stands as a trace of\nthe dear warband\na wonderously high wall,\nsnakingly woven.\nThe warriors have been snatched away\nby the honor of spears,\nslaughter-seeking weapons,\nthat famous fate,\nand storms clash against\nthese stone cliffs,\nfalling frost\ngrips the earth,\nthe clamor of winter.\nThen darkness comes;\nnight\u0026rsquo;s shadow deepens.\nFrom the north approaches\na rough hail storm,\nhostile to men.\nAll is troubled\nin this earthly kingdom,\nthe tide of fate\nturns the world under the heavens.\nHere is wealth fleeting.\nHere are companions fleeting.\nHere is man himself fleeting.\nHere is kinsman fleeting.\nEvery foundation of this world\nbecomes void and vain.\nSo spoke the wise one in his mind.\nHe sat apart in thought.\nGood is he who holds onto his faith,\nand a warrior must never speak\nthe grievances of his heart too quickly,\nunless he already knows the cure.\nA hero must act with conviction.\nGood is he who seeks mercy,\nthe Father\u0026rsquo;s grace in Heaven,\nwhere permanence stands for us all.\n","date":1562976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562976000,"objectID":"7cb75fac1a0293b61712ea00103fdafb","permalink":"http://lambdaviking.com/post/the-wanderer/","publishdate":"2019-07-13T00:00:00Z","relpermalink":"/post/the-wanderer/","section":"post","summary":"Translation of the Old English poem *The Wanderer*.","tags":null,"title":"The Wanderer","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://lambdaviking.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Will Merrill"],"categories":["NLP"],"content":" Capsule networks are a sophisticated neural network architecture originally proposed in the domain of computer vision. Motivated by some of the main problems with convolutional neural networks for image classification, capsule layers allow low-level features about an image to be effectively combined into higher-level information. In linguistic terms, we might call this compositionality. Given the fact that compositionality exists in natural language, we might wonder whether this kind of architecture could be useful for NLP tasks. Three recent papers have shown that capsule networks do in fact perform well on text classification tasks. One of the other big takeaways from these papers is that the capsule representation allows for effective transfer learning.\nCapsule Network Architecture For a more extensive discussion motivating the capsule architecture, see this blog post.\nConvolutional Networks To understand how capsule networks are designed, we should first consider standard convolutional networks. Convolutional networks are essentially a cascade of convolutional layers and max-pooling layers. Convolutional layers slide a filter across an image to look for different patterns, and pooling layers consolidate information from the previous layer into a more condensed form.\nWhile this convolutional architecture has achieved tremendous success on image classification and other tasks, it suffers from some severe limitations. For one, it takes a tremendous amount of data to learn rotational and reflectional invariants of high-level features (for example, faces). This is because we essentially need to learn a filter corresponding to each possible rotation of a feature. Another problem is that max-pooling loses information about the relative position of objects. For example, imagine we have a face where the positions of the eye and mouth are swapped. If max-pooling collapses the facial region down to one pixel, then, to the next layer of the network, it will be impossible to distinguish this distorted face from a normal one. Thus, we can create a variety of wacky faces that will fool convolutional networks but not humans. To solve this problem, we would want some model that enforces relative agreement between features.\n   Problems with Transformations Craziness with Max-Pooling          Capsules Enter capsule networks! The main idea behind a capsule network is that we replace each scalar-valued filter in a convolutional network with a vector-valued capsule. We can think of the value of a filter in a convolutional network as a boolean which tells as whether or not a feature exists there. On the other hand, because a capsule is a vector, it can encode not just this probability, but also properties (called instantiation parameters) of the feature. More specifically, for a capsule $u$:\n The magnitude of $u$ (between 0 and 1) gives a probability for the existence of the capsule feature. The direction of $u$ (in a high dimensional space) encodes different parameters of the capsule.  Connecting Capsule Layers Connecting layers of a capsules is a little more complicated than matrix multiplication in a conventional neural network. The existence of instantiation parameters allows us construct layer connection that enforce a learned notion of agreement. This is done in a two-step process:\n Compute vote vectors between each capsule $i$ in the previous layer and each capsule $j$ in the following layer. Intuitively, these vote vectors encode what information should be passed from $i$ to $j$. Run an iterative algorithm called dynamic routing to calculate the next layer\u0026rsquo;s capsules from the vote vectors. This algorithm is meant to enforce agreement, and uses no parameters.  An important fact to highlight is that, just like in a standard neural network, we can get different kinds of layers by applying routing in different ways. For example, we can get a convolutional capsule layer if we route from a sliding window in the previous layer, or a feedforward capsule layer if we route from all the capsules in the previous layer. Another important type of capsule layer is a primary capsule layer, which converts normal neural network values into capsules. This is always applied at the beginning of a capsule network.\nThere are a lot more details here. You can check out my slides or writeup from Dragomir Radev\u0026rsquo;s advanced NLP seminar, as well as the other resources which are linked there.\nCapsules Networks for NLP I will discuss three papers from 2018 that apply capsule networks to NLP tasks. I expect there will be more work published soon on this exciting and cutting edge area of research.\nInvestigating Capsule Networks with Dynamic Routing for Text Classification This paper was the first work to apply capsule networks to a text classification task. They evaluate the performance of a capsule network classification model on a variety of single-class and multi-class classification tasks. The multi-class data sets have single-class training data and multi-class evaluation data, so to perform well, the model must learn to transfer knowledge effectively from the single-class to multi-class case.\nA major contribution of the paper is to develop a capsule network architecture for text classification. This architecture, which is largely adapted by the other papers, can be summarized as follows:\n Standard convolutional layer Primary capsule layer Convolutional capsule layer (Flatten capsule matrix into capsule vector) Feedforward capsule layer  The results section compares the performance of this capsule architecture to many different strong baseline methods. In single-class classification, the standard capsule network architecture performs comparably to the best baseline methods. The authors also try a three-headed variant of their capsule network where three network computations are performed in parallel and then averaged, and this performs marginally better than the baselines. In the multi-class case, both capsule methods outperform the baselines, with the greatest increases again reported for the three-headed network. This suggests that the capsule representation is very effective at transferring knowledge from the single-class to multi-class problem.\nIdentifying Aggression and Toxicity in Comments using Capsule Network This paper focuses on a specific application of text classification: namely, classifying toxic comments. They look at two data sets for this task: one of comments in English, and the other which mixes English and Hindi. They hypothesize that the capsule architecture will be particular helpful in the code-mixed case because the local representations built up by a capsule network should handle mixing better than recurrent sequence models like LSTMs.\nThe authors approach this task using an architecture similar to the one used by Zhao, et al. They make two modifications to the architecture. First, they use an LSTM for the initial feature extraction layer instead of a convolutional network. Second, they replace the final feed-forward capsule layer with a standard feed-forward layer. The reason for this is so that they can use a focal loss function, with which they hope to overcome the class imbalance problems inherent to toxic comment classification.\nAgain, the authors compare their capsule architecture to a variety of competitive baseline methods. On both data sets, the capsule architecture outperforms the baseline methods. On the English data set, they achieve 98.46% accuracy, which is roughly 0.2% greater than the next best method. On different versions of the code-mixed data set, they report accuracies of 63.43% and 59.41%, which in both cases constitutes an increase of roughly 1% over the next-best method. These results imply that the capsule architecture is especially effective at classifying toxic comments when they are code-mixed.\nZero-shot User Intent Detection via Capsule Neural Networks This paper applies a similar capsule method to the classification task of zero-shot intent detection. Standard intent detection is the task where a dialog system must classify the an actionable intent that a user\u0026rsquo;s query represents from a known set of intents. For example, \u0026ldquo;Play a song!\u0026rdquo; might be classified as play_music. The zero-shot problem is more complicated: in this case, we have access to a set of known intents during training, but, at inference time, our goal is to classify from a set of previously unencountered intents. For example, imagine that we already trained a model on data containing the play_music intent, but our model has never seen play_movie. The goal is that we can still map \u0026ldquo;Play a movie!\u0026rdquo; to play_movie. Doing this requires the model to generalize what it has learned about the known intents to the unknown ones.\nTo do this, the authors devise an architecture that consists of three modules: two which are present during training and whose output capsules are a prediction over known classes, and a third one which attempts to generalize from these intermediate output capsules to unknown intents. The first two modules together function similarly to the base model developed by Zhao et al., except that the feature extraction layer is an LSTM with self-attention. These two modules are trained to classify known intents. The major architectural difference is the third untrained module for zero-shot inference. First, the authors compute a similarity matrix between known intents and unknown intents by using word embeddings for the words in the intent names. Then, they compute new vote vectors for the zero-shot layer by taking a weighted average of the vote vectors from the layer classifying known intents. They then use standard dynamic routing to convert these vote vectors into a layer of classification capsules for the unknown intents. Overall, I find this to be a really clever way to exploit the lack of parameters in dynamic routing for zero-shot learning.\nThe experimental results show that the known intent detection module by itself outperforms baselines for known intent detection across two different data sets. However, the key result is that the zero-shot intent detection network significantly outperforms the previous baseline systems against which it is compared. Thus, the authors are able to effectively utilize the capsule network architecture to do zero-shot inference.\nTakeaways Viewed together, these papers develop a standard capsule network architecture that can be used for text classification tasks. Each paper shows how a capsule network performs comparably to or better than the previous state of the art on a text classification task. Additionally, the papers leverage the representational power of capsules for effective transfer learning. The potential for transfer learning might be one of the most attractive things about the capsule network architecture for NLP tasks. Another exciting direction for future work is the adaptation of capsule networks to NLP tasks besides text classification.\n","date":1543449600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543449600,"objectID":"11366a7fed977ff717a22c24a85a8b1b","permalink":"http://lambdaviking.com/post/capsule-networks-for-nlp/","publishdate":"2018-11-29T00:00:00Z","relpermalink":"/post/capsule-networks-for-nlp/","section":"post","summary":"Review of 2018 literature on capsule networks for NLP.","tags":null,"title":"Capsule Networks for NLP","type":"post"},{"authors":["Yiding Hao","Will Merrill","Dana Angluin","Robert Frank","Noah Amsel","Andrew Benz","Simon Mendelsohn"],"categories":null,"content":"","date":1540857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540857600,"objectID":"8b3382a3037f95a217c09c9931f5589e","permalink":"http://lambdaviking.com/publication/context-free-transductions-with-neural-stacks/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/context-free-transductions-with-neural-stacks/","section":"publication","summary":"This paper analyzes the behavior of stack-augmented recurrent neural network models.","tags":["Source Themes"],"title":"Context-Free Transductions with Neural Stacks","type":"publication"},{"authors":["Will Merrill"],"categories":["NLP"],"content":" I\u0026rsquo;ve decided to share a literature review blog post of Learning to Transduce with Unbounded Memory (Grefenstette et al., 2015). I\u0026rsquo;ve found that blog posts summarizing literature have been helpful to me in the past, and after having written this review for a class, I thought it might be useful to post it for others.\nSummary This paper introduces a formalism for a differentiable stack data structure. The fact that this stack is differentiable allows it to be integrated into standard neural network models, which opens up the possibility for new neural network architectures that learn how to use a stack to solve a certain task.\nThe linguistic intuition underlying this development is that stacks are a useful data structure for parsing hierarchical (i.e. context-free) structure. The state-of-the art model for many linguistic tasks is some variant of an LSTM, which is a model that has only unstructured memory. Thus, by explicitly structuring the memory available to the model into a stack, the model should be able to learn a richer representation of natural language syntax. In addition to hopefully improving performance, having stack memory should also have the affect of making the algorithm that the neural network is learning more interpretable. This is important because interpretability is a major challenge for neural network models.\nThe authors first present their formalism for a neural stack. This formalism is a major development because it allows the network to interact with an unbounded amount of structured memory while also being almost-everywhere differentiable. In this context, differentiability means that the values read off from the stack are a differentiable function of all the values that have been pushed on to it. This differentiability condition is sufficient for being able to train the neural network using existing optimization methods.\nAfter proposing their differentiable stack formalism, the authors report results from neural stack networks applied to several toy linguistic tasks. The tasks considered are sequence copying, sequence reversal, bigram flipping, and toy languages meant to mimic SVO to SOV conversion and gender inflection in natural languages. On each of these tasks, a neural model augmented with a differentiable data structure does not just outperform the standard LSTM model, but converges to 100% accuracy.\nThis paper presents a linguistically motivated formalism for language tasks and show empirically that it can outperform standard methods on some toy tasks. Both the theoretical development of the architecture and the results are very interesting. They do not attempt to analyze how their networks are utilizing the differentiable data structure, which is a question that more recent research has gone on to address. Another open question is how these stack models would perform on natural language (not constructed) data sets.\nThis paper was very influential on me and others at the CLAY lab. If you are interested in further work that has been done on differentiable stacks, check out my recent publications.\n","date":1537833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537833600,"objectID":"3bdd54587a2326efc0086d69ab0167e2","permalink":"http://lambdaviking.com/post/learning-to-transduce-with-unbounded-memory/","publishdate":"2018-09-25T00:00:00Z","relpermalink":"/post/learning-to-transduce-with-unbounded-memory/","section":"post","summary":"Review and discussion of Grefenstette et al., 2015.","tags":null,"title":"Learning to Transduce with Unbounded Memory","type":"post"},{"authors":["Will Merrill"],"categories":["NLP"],"content":"Last semester in school, I worked on a project where we attempted to build word embeddings from the Voynich manuscript and use them to learn something about the linguistic structure (or lack thereof) within the text. You can check out our codebase or our paper. The reason why this methodology is exciting is that building word embeddings is completely unsupervised. This makes it a very appealing methodology for decipherment-like tasks. If done properly, we should be able to visualize the word embedding space and understand something about the relationship between words and characters in the manuscript!\nHowever, several issues make this methodology more complicated for Voynich. First, there are problems with transcription and word and sentence segmentation. Several digitalized transcriptions of the manuscript exist, and they disagree in a surprising number of places. A second concern is the issue of data sparsity. Canonical methods for word embeddings are designed to be architecturally simple so that they can be applied to massive data sets (for example, all of Wikipedia). These models overcome their architectural shortcomings from being hit over the head with a lot of data: there is no inductive bias that tells them that relocate and relocates are systematically related, but if the model gets to see all of Wikipedia, it should be able to figure this out. On the other hand, in a low-resource setting like Voynich, the distribution of words is sparse enough that we probably will never see every form of each verb. We need a more complex model that has the ability to learn a generalizable notion of morphology. It seems feasible that there is enough signal in the data for a model to actually learn a meaningful model of Voynich morphosyntax even if the data is too sparse to build a meaningful representation of the syntactic space.\nThere\u0026rsquo;s not much we can do to address the transcription concern except being principled about which transcription we adopt. However, there is recent work about building morphologically-informed word embeddings. In particular, our project utilized fastText embeddings, which were developed by Facebook research. FastText works by learning different embeddings for different subsequences of words and representing the embedding for the full word as a combination of these sub-embeddings. Intuitively, subsequences that are morphemes should contribute a meaningful vector to the full word vector, and subsequences that do not encode morphological information will have a representation close to zero. You can find the code to build these embeddings in the voynich2vec repository.\nOur original project tried to analyze these vectors in three different ways:\n Morphosyntactic analysis. We looked at how words with common suffixes were distributed in the Voynich embedding space. If a suffix contributes to where the word can appear (i.e., it carries morphosyntactic information), then we would expect words ending in it to cluster closely. Conversely, if it does not contribute to where the word can appear, this should mean that it is just a common suffix but not a morpheme. Excitingly, we found that some suffixes displayed said clustering, but others did not. Also interestingly, we found that some similar suffixes clustered together, which suggests that they are two different surface realizations of the same morpheme. Topic modelling. We can build a bag-of-words representation of some section of the manuscript by summing the embeddings for the words on that page. This sum could possibly be weighted by a metric like TF-IDF. We did this and got some clustering by section, but nothing very conclusive. Unsupervised word alignment. We attempted to use a method called MUSE to align two word embedding spaces. If this worked, this could give as an approximate mapping from Voynichese words to Latin words. Sadly, it did not work. Unsupervised alignment methods like MUSE rely on massive corpora. In our case, Voynich was so small that we only got noise. There has also been work done on semi-supervised word embeddings where two embedding spaces can be fully aligned from a small set of seed words (for example, just the month names).  Sadly, I haven\u0026rsquo;t been able to work on this project much since the beginning of the summer, but I\u0026rsquo;m keeping track of a central list of ideas for future work here. In particular, I think the most promising question is to extend our preliminary work on morphosyntactic analysis. As I describe in one of the GitHub issues, methods have been developed in the NLP literature for inducing morphology from a vector space of word embeddings. This method could be used to generate a full morphological profile of Voynichese.\n","date":1537833600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1537833600,"objectID":"9fd756e77fd4a2e1fb1ed80c6e24a87b","permalink":"http://lambdaviking.com/post/word2vec-analysis-of-the-voynich-manuscript/","publishdate":"2018-09-25T00:00:00Z","relpermalink":"/post/word2vec-analysis-of-the-voynich-manuscript/","section":"post","summary":"Can NLP help us decipher the enigmatic manuscript?","tags":["Voynich"],"title":"Word2vec Analysis of the Voynich Manuscript","type":"post"},{"authors":["Will Merrill"],"categories":["Translation","Old Norse"],"content":" Ragnarok is a well known part of the Norse mythological canon presented by Snorri Sturluson. But what happens after the world is destroyed? Gylfaginning actually tells us, and I\u0026rsquo;ve translated the corresponding section below. Check it out!\nTranslation Then Gangleri says, âAre any gods alive then? Or is there then any earth or heavens?â\nThe Grey One says, âUp shoots the Earth from the sea and it is then green and fertile. Unsown fields grow there. Vidarr and Vali live such that the sea and Surtâs flame have not hurt them, and they flourish there on Idavollr, where Asgard was before. And then the sons of Thor, Modi and Magni, come there, and they have there Mjollnir. Next, Baldr and Hodr come there from Hell. Everyone then sets themselves together and they talk among themselves and remember their secrets and speak of those times which had been in days gone by. Of the Midgard Serpent and of Fenris Wolf. Then they find themselves in that golden grass that the Aesir had had. So it is said:\n Vidarr and Vali\nflourish as gods\nwhen Surtâs flame burns.\nModi and Magni\nshall have Mjollnir\nafter Vingnirâs battle-blow.\n And there where is called the forest of Hoddmimir, two hide in Surtâs flame who are called Lif and Leifthrasir, and they have morning dew for food. And from these people comes so great a lineage that the whole world rises again. As is said here:\n Lif and Leifthrasir,\nbut they will hide\nin the wood of Hoddmimir.\nMorning dew\nThey will have for their food,\nAnd from there centuries will be nourished.\n And it will seem strange to you when the sun has gotten a daughter no less beautiful than she is, and she carries herself on the path of her mother, as it says here:\n The lone daughter carries Alfrodul;\nFenrir chases the other one.\nSo shall it tremble\nwhen the gods die,\nThe mother bears a daughter.\n But now if you should ask forth further, then I donât know from where it could come to you, since I have heard no man speak more about the path of time. Enjoy now what you have gotten.â\nNote: The Edda is framed as a prophecy about what is to come for the Aesir, who are, according to Snorri, legendary humans that fled Troy to establish a kingdom in Scandinavia. At this point, we leave the prophecy and go back to the setting where it is first being told.\nRight then Gangleri heard a great din some way from himself, and went out onto a hill to see. And when he could see more around him, he stands out in the flat field to see no hall and no city. He walked then and travelled his road and came home into his kingdom and said those tidings which he had seen and heard. And after him each man told these stories to the others.\nAnd the Aesir set themselves then into legend, ruled their kingdom, and remembered from all these prophecies which were told to them, and gave the same names which before were named to men and the places where they were, until a long time passed such that men could not remember that they all were the same, those Aesir who now were talked about, and these ones who then were given those same names. There was then the one called Thor, who is also Asathor the Old, who is Okuthor, and by him were known those great works which Thor (Hector) did at Troy. And men think that the Trojans had talked about Ulysses and had called him Loki, because The Trojans were his greatest of unfriends.\n","date":1530748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530748800,"objectID":"b6bdb0dad11a84f731c8b7e260eff83a","permalink":"http://lambdaviking.com/post/after-ragnarok/","publishdate":"2018-07-05T00:00:00Z","relpermalink":"/post/after-ragnarok/","section":"post","summary":"My translation of part of the *Prose Edda*.","tags":["Mythology"],"title":"After Ragnarok","type":"post"},{"authors":["Jungo Kasai","Robert Frank","Pauli Xu","Will Merrill","Owen Rambow"],"categories":null,"content":"","date":1528243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528243200,"objectID":"fd52ffeba577cd4dd21d04b2ed09825d","permalink":"http://lambdaviking.com/publication/end-to-end-graph-based-tag-parsing-with-neural-networks/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/end-to-end-graph-based-tag-parsing-with-neural-networks/","section":"publication","summary":"We present a graph-based Tree Adjoining Grammar parser that uses BiLSTMs, highway connections, and character-level CNNs.","tags":["Source Themes"],"title":"End-to-End Graph-Based TAG Parsing with Neural Networks","type":"publication"},{"authors":["Will Merrill"],"categories":["Translation","Old Norse"],"content":" In order to practice my Old Norse, I decided to write up a thorough translation of a short excerpt from the Icelandic Saga of Mary. I\u0026rsquo;ve given the original text underneath, which comes from A New Introduction to Old Norse edited by Anthony Faulkes. Please let me know if you have any corrections or suggestions! If I post more translations here, I will try to choose longer and more interesting passages.\nTranslation There was a monastery in that mountain which is called Tumba. There at the monastery stood a church of Michael. In the temple was a likeness of Mary, and it was made such that the Lord sat at her knees, and there was a long silk garment over its head. There came often great thunder and lightning, and a loose bolt hit the church so that it all burned down, but the likeness of Mary was unscathed, as was the pedestal on which it stood. Nowhere was the silk garment that was on the likeness damaged! The monks proclaimed this a miracle, and everyone who heard praised God. We have to pray to God that he free us from evil fire like he did the likeness from this fire-heat.\nOriginal MunklÃ­fi eitt var Ã­ fjalli Ã¾vÃ­, er Tumba heitir. Ãar stÃ³Ã° MikjÃ¡ls kirkja hjÃ¡ munklÃ­finu. Ã musterinu var MarÃ­u lÃ­kneskja, ok svÃ¡ ger sem DrÃ³ttinn sÃ¦ti Ã­ knjÃ¡m henni, ok var silkidÃºkr breiddr yfir hÃ¶fuÃ° Ã¾eim. Ãar kÃ³mu opt reiÃ°ar stÃ³rar ok eldingar, ok laust eitt sinn svÃ¡ kirkjuna, at hon brand Ã¶ll, en lÃ­kneskja MarÃ­u var heil, ok svÃ¡ stallrinn, er hon stÃ³Ã° Ã¡. Hvergi var Ã¡ silkidÃºkinn runnit, er Ã¡ lÃ­kneskjunni var. Munkar lÃ½stu Ã¾essi jartegn, ok lofuÃ°u allir GuÃ°, Ã¾eir er heyrÃ°u. VÃ©r eigum Ã¾ess GuÃ° at biÃ°ja, at hann leysi oss svÃ¡ frÃ¡ eilÃ­fum eldi sem lÃ­kneskit frÃ¡ Ã¾essum eldshita.\nRemarks The passage describes a miracle in which a statue of Mary struck by lightning does not burn. This event is meant as an allegory for the fate of Christian souls after death: the statue\u0026rsquo;s unburned state represents God\u0026rsquo;s ability to spare mankind from hellfire.\nOne odd thing I noticed is that feminine \u0026ldquo;lÃ­kneskja\u0026rdquo; in the description of the miracle becomes neuter \u0026ldquo;lÃ­kneskit\u0026rdquo; in the final sentence that injects the allegorical interpretation. I can\u0026rsquo;t think of any rhetorical significance that this could have though.\n","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"a04184b42e635d9c63f790d271a294e0","permalink":"http://lambdaviking.com/post/saga-of-mary/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/post/saga-of-mary/","section":"post","summary":"Translated excerpt from the Old Icelandic *Saga of Mary*.","tags":null,"title":"The Saga of Mary","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"http://lambdaviking.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"http://lambdaviking.com/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]