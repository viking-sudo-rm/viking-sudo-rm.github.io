<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Will Merrill</title>
    <link>http://lambdaviking.com/</link>
    <description>Recent content on Will Merrill</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    
	    <atom:link href="http://lambdaviking.com/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Recent &amp; Upcoming Talks</title>
      <link>http://lambdaviking.com/talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/talk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Miscelleneous</title>
      <link>http://lambdaviking.com/misc/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0100</pubDate>
      
      <guid>http://lambdaviking.com/misc/</guid>
      <description>&lt;p&gt;[wɪl mɛɹl] ±ɛ/æ distinction&lt;/p&gt;

&lt;p&gt;I like dead languages. I&amp;rsquo;m decent at Latin, Old Norse, and Old English.
&lt;!-- Strangely enough, I took a class where we learned Proto Indo European. --&gt;
I&amp;rsquo;ve also dabbled in Middle Egyptian, and considering studying Nahuatl.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m a big proponent of the urban walk. People in Seattle seem to prefer hiking, which is fun too I guess?&lt;/p&gt;

&lt;p&gt;I play pick-up basketball. I also play and have (amateurly) made video games. Check out my &lt;a href=&#34;https://www.youtube.com/snorridevteam&#34; target=&#34;_blank&#34;&gt;YouTube channel&lt;/a&gt; from when I was 15 ¯\_(ツ)_/¯&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s some random stuff that I&amp;rsquo;ve written over the years, in no particular order.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://lambdaviking.com/files/NavajoCodeTalkers.pdf&#34;&gt;Origin of the Navajo code talkers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lambdaviking.com/files/Jorsalafari.pdf&#34;&gt;&lt;em&gt;Jórsalafari&lt;/em&gt;&lt;/a&gt;: the Norwegian Crusade as a viking expedition&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.packerintersections.com/the-history-of-minecraft-how-a-swedish-indie-game-came-to-dominate-the-world.html&#34; target=&#34;_blank&#34;&gt;The history of Minecraft&lt;/a&gt;: how a Swedish indie game came to dominate the world&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://lambdaviking.com/files/dantepp.pdf&#34;&gt;Dante++&lt;/a&gt;: A chapter extending &lt;em&gt;Inferno&lt;/em&gt; to the age of Big Data™ and AI&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*There&amp;rsquo;s other stuff on my &lt;a href=&#34;http://lambdaviking.com/post/&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
<<<<<<< HEAD
      <title>Publications</title>
      <link>http://lambdaviking.com/publication/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/publication/</guid>
=======
      <title>A Formal Hierarchy of RNN Architectures</title>
      <link>http://lambdaviking.com/post/rr-hierarchy/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/post/rr-hierarchy/</guid>
      <description>

&lt;p&gt;This post is about &lt;a href=&#34;https://arxiv.org/abs/2004.08500&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;A Formal Hierarchy of RNN Architectures&lt;/em&gt;&lt;/a&gt;, which was joint work between myself, Gail Weiss, Yoav Goldberg, Roy Schwartz, Noah Smith, and Eran Yahav. Compared to the original paper, this post is more of a summary, and will attempt to explain the significance of the results assuming less familiarity with formal language theory.&lt;/p&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;Understanding the practical capacity of neural network architectures is an important question for both the design of new architectures and the interpretability of current ones. By &amp;ldquo;practical capacity&amp;rdquo;, we mean the classes of tasks that an architecture can discover solutions to via standard training methods. Since we are mostly interested in NLP here, another way to describe this is the types of grammars that a trained neural network can learn to implement. It has been known since &lt;a href=&#34;https://dl.acm.org/doi/10.1145/130385.130432&#34; target=&#34;_blank&#34;&gt;Siegelmann and Sontag (1992)&lt;/a&gt; that RNNs with infinite time and precision are Turing-complete; however, these unrealistic assumptions make this a bad formal model for practical learnable capacity.&lt;/p&gt;

&lt;p&gt;Over the last year or two, several works have addressed this by relating deep NLP architectures like RNNs to existing formal models in automata theory. &lt;a href=&#34;https://arxiv.org/abs/1805.04908&#34; target=&#34;_blank&#34;&gt;Weiss et al. (2018)&lt;/a&gt; showed a connection between LSTMs and counter machines (CMs), and demonstrated how LSTMs learn to count to solve certain formal tasks that other RNNs cannot solve. Building on this, &lt;a href=&#34;https://arxiv.org/abs/1906.01615&#34; target=&#34;_blank&#34;&gt;Merrill (2019)&lt;/a&gt; formalized the notion of a saturated network (a finite-precision approximation of a continuous RNN) and related the capacity of different saturated RNNs to different classes of formal languages. &lt;a href=&#34;https://arxiv.org/abs/1808.09357&#34; target=&#34;_blank&#34;&gt;Peng et al. (2018)&lt;/a&gt; explored RNN capacity along a different axis: whether or not their internal computation can be simulated by a weighted finite state machine (WFA).&lt;/p&gt;

&lt;h3 id=&#34;the-hierarchy&#34;&gt;The Hierarchy&lt;/h3&gt;

&lt;p&gt;The goal of this paper is to unify these independent threads of research by further exploring the connection between formal models like saturated RNNs, CMs, and WFAs. We place all of these models into a two dimensional hierarchy defined by two formal properties: rational recurrence and space complexity. As defined by &lt;a href=&#34;https://arxiv.org/abs/1808.09357&#34; target=&#34;_blank&#34;&gt;Peng et al. (2019)&lt;/a&gt;, an RNN is &lt;em&gt;rationally recurrent&lt;/em&gt; if its recurrent state can be computed by a WFA. &lt;em&gt;Space complexity&lt;/em&gt;, related to the concept in analysis of algorithms, measures how much memory is available to a model.&lt;/p&gt;

&lt;p&gt;We present new results characterizing models in terms of these properties. For example, we prove the saturated LSTM is not rationally recurrent, which was previously an open question. We also show that general CMs are not rationally recurrent. However, we explore restricted classes of counter machines that are. While the main focus of the paper is on RNNs, we also present results analyzing memory networks and transformers in the same terms. Together, these results provide a hierarchy of RNNS and related models in terms of the functions that their hidden states can express.&lt;/p&gt;

&lt;h2 id=&#34;language-expressiveness&#34;&gt;Language Expressiveness&lt;/h2&gt;

&lt;p&gt;Once we have derived these characterizations, we use them to demonstrate formal languages that separate the capacities of different RNNs. We add a 1 or 2-layer feedforward &amp;ldquo;pooler&amp;rdquo; after the final RNN state and view this full model as a language acceptor. Here are some of the results in this vein, stated more formally. Let $D_k$ denote the capacity of an RNN with a $k$-layer pooler, and let s-$X$ denote the saturated version of architecture $X$.&lt;/p&gt;

&lt;p&gt;$$ a^nb^n \in D_1(\textrm{s-LSTM}) $$
$$ a^nb^n \not\in D_1(\textrm{s-QRNN}) $$&lt;/p&gt;

&lt;p&gt;The QRNN with a 1-layer pooler cannot recognize $a^nb^n$, whereas the LSTM can.&lt;/p&gt;

&lt;p&gt;$$ a^nb^n \in D_1(\textrm{WFA}) $$&lt;/p&gt;

&lt;p&gt;While the rationally recurrent s-QRNN cannot recognize $a^nb^n$, a WFA actually can! We provide an existence proof using some linear algebra, and then provide a method through which the proof can be extended to construct an example WFA.&lt;/p&gt;

&lt;p&gt;$$ a^nb^n \in D_2(\textrm{s-QRNN}) $$&lt;/p&gt;

&lt;p&gt;Adding a second linear layer (or using two s-QRNN layers) allows us to recognize $a^nb^n$ with an s-QRNN. Does this mean the hierarchy dissolves as the pooler is strengthened?&lt;/p&gt;

&lt;p&gt;$$ a^nb^n\Sigma^* \in D_1(\textrm{s-LSTM}) $$
$$ a^nb^n\Sigma^* \not\in D(\textrm{s-QRNN}) \; \textrm{for any $D$} $$&lt;/p&gt;

&lt;p&gt;No, the hierarchy persists, even for stronger decoders. We provide a simple extension of $a^nb^n$ that can be recognized by an s-LSTM with a 1-layer pooler, but can never be recognized by an s-QRNN, no matter how many layers the pooler has. We dub the technique used to prove this the &lt;em&gt;suffix attack&lt;/em&gt;. It exploits the fact that the QRNN (compared to the LSTM) is fundamentally unable to detect when its state has reached an accepting configuration&amp;ndash;thus, it cannot &amp;ldquo;stop&amp;rdquo; updating when the strings are padded with an arbitrary suffix. Since this is a general property, it can be directly adapted to other formal languages.&lt;/p&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;Finally, we run experiments testing whether unsaturated networks trained by gradient descent can learn variants of $a^nb^n$ and $a^nb^n\Sigma^*$. In every case, we find that the capacity of the saturated networks correctly predicts the outcome of the experiments. Thus, while our theoretical results largely focus on saturated networks, it seems that they also describe the practical behavior learned by unsaturated ones.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Theory of Saturated Neural Networks</title>
      <link>http://lambdaviking.com/post/saturated-networks/</link>
      <pubDate>Sat, 26 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/post/saturated-networks/</guid>
      <description>

&lt;p&gt;The main idea of my paper &lt;a href=&#34;https://arxiv.org/abs/1906.01615&#34; target=&#34;_blank&#34;&gt;Sequential Neural Networks as Automata&lt;/a&gt; is that, if we making some simplifying assumptions about how neural networks work, we can derive a theory of network expressiveness (what formal languages can different architectures model?) that seems to agree with the classes of formal languages that different networks can learn when trained by gradient descent. Thus, this restricted theoretical capacity seems to be (potentially) a good proxy for the empirical learnable capacity of various networks.&lt;/p&gt;

&lt;h2 id=&#34;saturated-networks&#34;&gt;Saturated Networks&lt;/h2&gt;

&lt;p&gt;In the paper, I referred to the simplified network whose capacity we can analyze as an &lt;em&gt;asymptotic&lt;/em&gt; network. However, after talking with Gail Weiss, I now believe the term &lt;em&gt;saturated&lt;/em&gt; is more descriptive, and plan to use this term going forward.&lt;/p&gt;

&lt;p&gt;A neural network is a function $f(x, \theta)$ that is almost-everywhere differentiable with respect to the parameters $\theta$. Given such a function, we derive the saturated network $f&amp;rsquo;$ as&lt;/p&gt;

&lt;p&gt;$$ f&amp;rsquo;(x, \theta) = \lim_{N \rightarrow \infty} f(x, N\theta) . $$&lt;/p&gt;

&lt;p&gt;We define $f&amp;rsquo;$ over the domain of $(x, \theta)$ for which the limit above exists.&lt;/p&gt;

&lt;p&gt;In a neural network, the effect of this transformation is to discretize all of the activations. For example, consider a neuron:&lt;/p&gt;

&lt;p&gt;$$ \sigma(wx + b) $$&lt;/p&gt;

&lt;p&gt;where $\sigma$ is the sigmoid function. When we take the limit of $\sigma(Nwx + Nb)$, the output of the neuron approaches either $0$ or $1$. This is what I mean by &lt;em&gt;discretization&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;After applying this discretization to the full network, we can analyze the computational capacity of the resulting discrete automaton. We also define a notion of space complexity associated with these saturated networks in the paper. Intuitively, this measure of complexity is just the number of configurations that the saturated network can have after reading a sequence of length $n$. For more details on this, consult &lt;a href=&#34;https://arxiv.org/abs/1906.01615&#34; target=&#34;_blank&#34;&gt;the paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;summary-of-results&#34;&gt;Summary of Results&lt;/h2&gt;

&lt;p&gt;By $L(M)$, we denote the set of formal languages that a machine $M$ can accept. Some key capacity results from the paper are as follows:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$L(\textrm{ConvNet})$ is a proper subset of the regular languages&lt;/li&gt;
&lt;li&gt;$L(\textrm{RNN})$ is exactly the regular languages&lt;/li&gt;
&lt;li&gt;$L(\textrm{GRU})$ is exactly the regular languages&lt;/li&gt;
&lt;li&gt;$L(\textrm{LSTM})$ is a superset of the regular languages, and a subset of the real-time counter languages&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The core results about the configuration complexity, some of which are analogous, are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ConvNet has $O(1)$ configurations&lt;/li&gt;
&lt;li&gt;RNN has $O(1)$ configurations&lt;/li&gt;
&lt;li&gt;GRU has $O(1)$ configurations&lt;/li&gt;
&lt;li&gt;LSTM has $O(n^k)$ configurations for hidden size $k$&lt;/li&gt;
&lt;li&gt;Attention has $2^{O(n)}$ configurations&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/viking-sudo-rm/stacknn-core&#34; target=&#34;_blank&#34;&gt;StackNN&lt;/a&gt; has $2^{O(n)}$ configurations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are some other results about attention/transformers that I&amp;rsquo;m not going to get into here, since they&amp;rsquo;re not so neat. If you&amp;rsquo;re interested though, I refer you to Section 4 of &lt;a href=&#34;https://arxiv.org/abs/1906.01615&#34; target=&#34;_blank&#34;&gt;the paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;empirical-evidence&#34;&gt;Empirical Evidence&lt;/h2&gt;

&lt;h3 id=&#34;positive-evidence&#34;&gt;Positive Evidence&lt;/h3&gt;

&lt;p&gt;The development of this theory was motivated by past empirical results about what RNNs are able to learn. Two types of tasks (counting and reversing) serve as relevant diagnostics for assessing the computational power of different architectures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.04908&#34; target=&#34;_blank&#34;&gt;Weiss et al.&lt;/a&gt; show that LSTMs can learn to count, whereas RNNs and GRUs do not. Similarly, &lt;a href=&#34;https://arxiv.org/abs/1906.03648&#34; target=&#34;_blank&#34;&gt;Sugzgun et al.&lt;/a&gt; observe that LSTMs can learn the 1-Dyck counter language, whereas other RNNs do not. This is predicted by the theory since saturated LSTMs can do counting, but saturated RNNs and GRUs are finite state.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1809.02836&#34; target=&#34;_blank&#34;&gt;Hao et al.&lt;/a&gt; show how stack neural networks can solve the (beyond counter language) task of reversing a string, whereas LSTMs fail badly on this. In my paper, I also showed that attention can solve this task. Both of the results are predicted by the theory, since stack neural networks and attention allow for exponential configurations, which are needed to reverse a string, whereas LSTMs are limited to polynomial configurations.&lt;/p&gt;

&lt;h3 id=&#34;negative-evidence&#34;&gt;Negative Evidence&lt;/h3&gt;

&lt;p&gt;While I found that regularized neural networks display the counting pattern reported by &lt;a href=&#34;https://arxiv.org/abs/1805.04908&#34; target=&#34;_blank&#34;&gt;Weiss et al.&lt;/a&gt;, I also found that the unregularized LSTM, GRU, and RNN can all learn to model the language $a^nb^nc$, which requires counting. Thus, it might be more precise to say that the saturated theory seems to describe the learnable capacity of &lt;em&gt;regularized&lt;/em&gt; networks. One possible interpretation of this is that the constraints imposed by regularization prevent the network from learning strategies beyond the saturated capacity.&lt;/p&gt;

&lt;h2 id=&#34;proof-sketches&#34;&gt;Proof Sketches&lt;/h2&gt;

&lt;h3 id=&#34;rnn-capacity-and-complexity&#34;&gt;RNN Capacity and Complexity&lt;/h3&gt;

&lt;p&gt;To show that $L(\textrm{RNN})$ is the regular languages, we show two directions of containment.&lt;/p&gt;

&lt;p&gt;First, we prove that the the regular languages are an upper bound. We do this by showing that the configuration complexity of the RNN is finite, i.e. $O(1)$. Since each neuron has two possible values ($-1$ and $1$), and there are $k$ neurons in the state, the number of configurations of the state vector is $O(2^k) = O(1)$.&lt;/p&gt;

&lt;p&gt;The other direction is a little more complicated. We need to construct an RNN to simulate an arbitrary finite state machine. A construction for this is provided in Lemma B.1.&lt;/p&gt;

&lt;h3 id=&#34;lstm-capacity-and-complexity&#34;&gt;LSTM Capacity and Complexity&lt;/h3&gt;

&lt;p&gt;In the LSTM case, even when we discretize the network, we get a model with more than finite state. This is because the LSTM&amp;rsquo;s gating architecture is capable of counting (&lt;a href=&#34;https://arxiv.org/abs/1805.04908&#34; target=&#34;_blank&#34;&gt;Weiss et al.&lt;/a&gt;, 2018).&lt;/p&gt;

&lt;p&gt;To show that the counter languages are an upper bound, we write the saturated gate network for a particular counter state neuron $c$ as follows:&lt;/p&gt;

&lt;!-- For some reason, the normal \lim_ is not working here. Neither does f_tc_{t-1}. I guess --&gt;

&lt;p&gt;$$ \underset{N \rightarrow \infty}{\lim} c_t $$&lt;/p&gt;

&lt;p&gt;$$ = \underset{N \rightarrow \infty}{\lim} fc_{t-1} + i{\tilde c}_t $$&lt;/p&gt;

&lt;p&gt;$$ = \underset{N \rightarrow \infty}{\lim} ac_{t-1} + b $$&lt;/p&gt;

&lt;p&gt;where $a$ is $0$ or $1$ and $b$ is $-1$, $0$, or $1$. This equation parameterizes a real-time counter machine update. Thus, the counter languages are an upper bound on the saturated LSTM capacity.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The State of Interpretability in NLP</title>
      <link>http://lambdaviking.com/post/the-state-of-interpretability-in-nlp/</link>
      <pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/post/the-state-of-interpretability-in-nlp/</guid>
      <description>

&lt;p&gt;This post is a summary of my thoughts and experiences at ACL 2019, focused around the theme of interpretability. After spending most of the summer thinking about things besides neural networks, I was abruptly thrown back into things upon arriving in Florence. I spent a lot of my downtime with friends from Yale that I was staying with. On the other hand, I also met my future colleagues at AI2 and many other young researchers who are interested in similar issues of the interpretability and formal analysis of neural networks. In this way, the conference felt like a soft threshold between old and new.&lt;/p&gt;

&lt;p&gt;There were plenty of exciting things going on at the main conference, from work on incorporated graph representations into transformers to the development of more energy-efficient non-autoregressive NLP models. In this post, though, I&amp;rsquo;m going to focus on parts that I was most directly involved in, which were the Blackbox NLP workshop on interpretability and the Deep Learning and Formal Languages (DELFOL) workshop.&lt;/p&gt;

&lt;h1 id=&#34;blackbox-nlp&#34;&gt;Blackbox NLP&lt;/h1&gt;

&lt;p&gt;This was the second iteration of the Blackbox workshop. While the last Blackbox at EMNLP was also quite large and interesting, it had felt to me like a quite interdisciplinary meeting of researchers doing very different things. In contrast, I got the impression at this year&amp;rsquo;s meeting that a community of interpretability-minded researchers with similar goals and methods has formed. This is consistent with the wider interest that interpretability has sparked in the larger NLP community and the number of interpretability papers presented in the main conference. In the closing panel of the workshop, it was mentioned that perhaps interpretability would become a session within the conference at the next ACL meeting instead of a supplementary workshop.&lt;/p&gt;

&lt;p&gt;One of the main goals of interpretability research is to assess what kinds of information is encoded in different representations within a neural network. In particular, people (including myself) are interested in how different models encode grammatical information. Several methods were discussed for addressing this question, including attention, probing, representational similarity analysis (RSA).&lt;/p&gt;

&lt;h2 id=&#34;attention&#34;&gt;Attention&lt;/h2&gt;

&lt;p&gt;A classical way of interpreting whether networks encode grammatical information is by looking at attention alignments. For example, we might expect that, if there is a dependency between words $i$ and $j$, then position $i$ in the network should attend to position $j$. While this probably works to some degree, attention maps can often get messy. Also, as &lt;a href=&#34;https://arxiv.org/abs/1906.03731&#34; target=&#34;_blank&#34;&gt;Serrano et al.&lt;/a&gt; pointed out in their work at the main conference, perturbing the attention distribution has surprisingly little effect on a network&amp;rsquo;s behavior. Thus, we might want to look to other methods for interpreting neural representations.&lt;/p&gt;

&lt;h2 id=&#34;probing&#34;&gt;Probing&lt;/h2&gt;

&lt;p&gt;Probing refers to using a linear classifier to try to recover some kind of information, say part of speech information, from some representation within a model. This has been a standard way of interpreting neural representations throughout the last year. The underlying assumption is that, if information is recoverable in a single linear layer, then it is probably strongly encoded in the representation. As was pointed out to me by &lt;a href=&#34;https://homes.cs.washington.edu/~jkasai/&#34; target=&#34;_blank&#34;&gt;Jungo Kasai&lt;/a&gt;, one pitfall of this method is that it just assesses whether or not the information of interest is encoded in a representation, not to what degree it is there. For example, if a small amount of the representation in some network is representing grammatical information, whereas the rest of it is representing spurious correlations between words, probing would still suggest that that layer is encoding grammar. In such a case, is it really reasonable to conclude that a grammatical representation is an important part of the behavior which the network is learning?&lt;/p&gt;

&lt;h2 id=&#34;rsa&#34;&gt;RSA&lt;/h2&gt;

&lt;p&gt;An alternate approach to the same problem is RSA, which is not to be confused with the cryptographic protocol of the same name. RSA sidesteps the issue with probing by measuring the degree to which two representations are correlated, not whether the information from one can be recovered from the other.&lt;/p&gt;

&lt;p&gt;Formally, RSA is a method of computing the similarity between two representations $X$ and $Y$. We assume that we do not have the direct ability to compare $x$ to $y$, but instead have two internally defined similarity measures $d_X(x_1, x_2)$ and $d_Y(y_1, y_2)$. These measures need not be symmetric.&lt;/p&gt;

&lt;p&gt;To get an overall measure of similarity, we compute a similar matrices $M(X)$ and $M(Y)$ where&lt;/p&gt;

&lt;p&gt;$$ M(X)_{ij} = d_X(x_i, x_j) $$&lt;/p&gt;

&lt;p&gt;and analogously for $Y$. Once we have these matrices, we compute an overall measure of similarity by taking the correlation between them. I find RSA to be a quite exciting method for future research, as it avoids some of the pitfalls of probing while also being very easy (perhaps easier than probing) to implement. This is not to say that probing should be abandoned completely, as being able to verify the same results with different methods increases their confidence. I recall hearing a talk at ACL in which RSA was already applied to transformer representations and replicated probing findings, although I unfortunately cannot find the paper which reported this.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Probing and RSA are two good methods for understanding what information is encoded in some representation within a trained network. This is still a slightly different question than asking what information is particular relevant for the decision that a network makes given some input, which can be addressed using gradient methods or perturbation studies. I think that interpreting representations and interpreting decisions are both interesting questions.&lt;/p&gt;

&lt;p&gt;Additionally, both of these questions have analogs in the formal domain, where we can analyze what representations different neural network hidden states can encode, or what classes of formal languages a certain architecture can accept. As others at ACL also mentioned, I would like to see future work connecting empirical interpretability research to the formal analysis that has been done.&lt;/p&gt;

&lt;p&gt;Finally, as many people expressed during the conference, I think that a good goal for the next year is progress towards constructive interpretability work. In other words, our interpretation of networks should allow us to make smarter architectures.&lt;/p&gt;

&lt;h1 id=&#34;delfol&#34;&gt;DELFOL&lt;/h1&gt;

&lt;p&gt;DELFOL was the venue for formal work at ACL 2019. Given its somewhat esoteric topic, this workshop was quite small, but I found it really exciting. To me, the following three questions captured a lot of the work that people were presenting:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Assess the learnable capacity of neural network models using empirical studies&lt;/li&gt;
&lt;li&gt;Identify formal properties of neural architectures that lead to different kinds of inductive bias or learnable/representational capacity&lt;/li&gt;
&lt;li&gt;&amp;ldquo;Distill&amp;rdquo; neural network models into smaller, more interpretable discrete models&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;empirical-experiments-about-learnability&#34;&gt;Empirical Experiments about Learnability&lt;/h2&gt;

&lt;p&gt;On the empirical front, &lt;a href=&#34;https://arxiv.org/abs/1906.03648&#34; target=&#34;_blank&#34;&gt;Suzgun et al.&lt;/a&gt; reported that LSTMs generalizably learn to model the 1-Dyck language, which consists of parenthetic expressions with one type of parentheses. Other types of RNNs, on the other hand, were not able to do this. Additionally, LSTMs could not model 2-Dyck, but could model independently mixed versions of 1-Dyck.&lt;/p&gt;

&lt;p&gt;Since 1-Dyck is a counter language, this is further evidence that LSTMs can count, unlike other kinds of RNNs. Additionally, since 2-Dyck is a context-free language that requires more memory than is available to counter machines, we have further experimental evidence that LSTM memory, like counter machine memory, is not sufficient for complex hierarchical representations. As &lt;a href=&#34;https://bobfrank1.github.io/&#34; target=&#34;_blank&#34;&gt;Bob Frank&lt;/a&gt; summarized in his invited talk at DELFOL, LSTMs also struggle to do other memory intensive context-free things, like reversing strings.&lt;/p&gt;

&lt;p&gt;While &lt;a href=&#34;https://arxiv.org/abs/1906.01615&#34; target=&#34;_blank&#34;&gt;my paper&lt;/a&gt; at DELFOL was mostly theoretical results, I also had some experimental stuff that agreed with these conclusions.&lt;/p&gt;

&lt;h2 id=&#34;formal-analysis-of-inductive-bias-and-capacity&#34;&gt;Formal Analysis of Inductive Bias and Capacity&lt;/h2&gt;

&lt;h3 id=&#34;saturated-networks&#34;&gt;Saturated Networks&lt;/h3&gt;

&lt;p&gt;Building on work done by &lt;a href=&#34;https://arxiv.org/abs/1805.04908&#34; target=&#34;_blank&#34;&gt;Weiss et al.&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1906.01615&#34; target=&#34;_blank&#34;&gt;my paper&lt;/a&gt; at DELFOL developed a theory of &lt;strong&gt;saturated networks&lt;/strong&gt;, or networks where the activation functions are restricted to their asymptotic values instead of intermediate rational values. I proved that saturated RNNs and GRUs are equivalent to finite-state automata in capacity, whereas saturated LSTMs are equivalent to a restricted class of counter machines. &lt;a href=&#34;https://bobfrank1.github.io/&#34; target=&#34;_blank&#34;&gt;Bob Frank&lt;/a&gt; also synthesized my results and Weiss et al.&amp;rsquo;s results in his invited talk.&lt;/p&gt;

&lt;p&gt;I think these theoretical results about saturated networks are interesting because they agree with empirical results (see previous section) about the types of languages that natural networks are empirically able to learn and generalize. This suggests that the capacity of the saturated network might represent something like the inductive bias or effective learnable capacity of the general network.&lt;/p&gt;

&lt;h3 id=&#34;other-formal-properties&#34;&gt;Other Formal Properties&lt;/h3&gt;

&lt;p&gt;In his talk, &lt;a href=&#34;https://bobfrank1.github.io/&#34; target=&#34;_blank&#34;&gt;Bob Frank&lt;/a&gt; also mentioned other ways of understanding the abilities of different networks. In particular, he mentioned studying the closure properties of representable or learnable languages as a potential way to build a theory. Another notable formal property that he mentioned was semilinearity. In his talk, &lt;a href=&#34;https://homes.cs.washington.edu/~nasmith/&#34; target=&#34;_blank&#34;&gt;Noah Smith&lt;/a&gt; mentioned rational recurrence as one way to analyze different neural architectures. While less linguistically motivated, this property is nice because of its connections to classical finite-state methods in NLP and the interpretability that comes along with this.&lt;/p&gt;

&lt;h3 id=&#34;distillation&#34;&gt;Distillation&lt;/h3&gt;

&lt;p&gt;Finally, a general theme at DELFOL that I found especially interesting was distillation. This refers to converting a neural network to a smaller and more interpretable discrete model which approximates the same behavior. This has a variety of practical benefits: deployment on smaller devices, interpretability, and possibly regularization, to name a few. I think one of the reasons why I find this topic interesting is because it is a way for theory to provide very concrete solutions to practical problems.&lt;/p&gt;

&lt;p&gt;As I see it, there are two main ways that distillation has been done. &lt;a href=&#34;https://arxiv.org/abs/1711.09576&#34; target=&#34;_blank&#34;&gt;Weiss et al.&lt;/a&gt; pioneered the &lt;strong&gt;query-based&lt;/strong&gt; approach, which involves using the &lt;a href=&#34;https://people.eecs.berkeley.edu/~dawnsong/teaching/s10/papers/angluin87.pdf&#34; target=&#34;_blank&#34;&gt;L* algorithm&lt;/a&gt; to learn a finite-state machine using black-box queries to a neural network. The other type of distillation is &lt;strong&gt;spectral&lt;/strong&gt;, which &lt;a href=&#34;https://pageperso.lis-lab.fr/~remi.eyraud/WP/&#34; target=&#34;_blank&#34;&gt;Rémi Eyraud&lt;/a&gt; spoke about at DELFOL. This involves building a Hankel matrix for some language and then using SVD to extract a smaller matrix representing a weighted finite-state automaton. While these are the two methods that have been investigated, I&amp;rsquo;m interested in other ways to achieve distillation, and I have been doing some preliminary experiments using other methods. More on that to come!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting Syntactic Change Using a Neural Part-of-Speech Tagger</title>
      <link>http://lambdaviking.com/publication/detecting-syntactic-change-using-a-neural-part-of-speech-tagger/</link>
      <pubDate>Fri, 02 Aug 2019 00:02:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/publication/detecting-syntactic-change-using-a-neural-part-of-speech-tagger/</guid>
>>>>>>> 8d2af2a27439710173a02b040f73b2b34f102bc2
      <description></description>
    </item>
    
    <item>
      <title>Posts</title>
      <link>http://lambdaviking.com/post/</link>
      <pubDate>Thu, 16 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/post/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>http://lambdaviking.com/slides/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/slides/</guid>
      <description></description>
    </item>
    
    <item>
<<<<<<< HEAD
      <title>Projects</title>
      <link>http://lambdaviking.com/project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
=======
      <title>Sense Abstraction: A Unified Framework of Intensionality, Predicate Abstraction, and Alternative Semantics</title>
      <link>http://lambdaviking.com/publication/sense-abstraction/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/publication/sense-abstraction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Saga of Mary</title>
      <link>http://lambdaviking.com/post/saga-of-mary/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
>>>>>>> 8d2af2a27439710173a02b040f73b2b34f102bc2
      
      <guid>http://lambdaviking.com/project/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link></link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid></guid>
      <description></description>
    </item>
    
    <item>
      <title>Courses</title>
      <link>http://lambdaviking.com/courses/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://lambdaviking.com/courses/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
